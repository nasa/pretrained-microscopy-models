{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "from itertools import cycle, product\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import albumentations as albu\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/segmentation/ebc_exp1/'\n",
    "\n",
    "x_train_dir = os.path.join(DATA_DIR, 'train')\n",
    "y_train_dir = os.path.join(DATA_DIR, 'train_annot')\n",
    "\n",
    "x_valid_dir = os.path.join(DATA_DIR, 'val')\n",
    "y_valid_dir = os.path.join(DATA_DIR, 'val_annot')\n",
    "\n",
    "x_test_dir = os.path.join(DATA_DIR, 'test')\n",
    "y_test_dir = os.path.join(DATA_DIR, 'test_annot')\n",
    "\n",
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MICRO_MODELS_DIR = './microscopynet_models/'\n",
    "\n",
    "# Dictionaries with model paths\n",
    "with open(os.path.join(MICRO_MODELS_DIR, 'microscopynet.csv'), mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    microscopynet_weights = {r[0]:r[1] for r in reader}\n",
    "    \n",
    "with open(os.path.join(MICRO_MODELS_DIR, 'microscopynet_fromscratch.csv'), mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    microscopynet_fromscratch_weights = {r[0]:r[1] for r in reader}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_1_4_shift(BaseDataset):\n",
    "    \"\"\"Read images, apply augmentation and preprocessing transformations.\n",
    "    Modified from https://github.com/qubvel/segmentation_models.pytorch\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_values (dict): values of classes to extract from segmentation mask. \n",
    "            Each dictionary value can be an integer or list that specifies the mask\n",
    "            values that belong to the class specified by the corresponding dictionary key.\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir, \n",
    "            class_values,\n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.ids = os.listdir(images_dir)\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
    "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
    "        \n",
    "        self.class_values = class_values\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        xs = random.randint(-256, 256) #row shift\n",
    "        ys = random.randint(-256, 256) #col shift\n",
    "        \n",
    "        # read data\n",
    "        if i%4 == 0:\n",
    "            image = cv2.imread(self.images_fps[i//4])[max(0,xs):512+max(0,xs), max(0,ys):512+max(0,ys)]\n",
    "            mask = cv2.imread(self.masks_fps[i//4], 0)[max(0,xs):512+max(0,xs), max(0,ys):512+max(0,ys)]\n",
    "        elif i%4 == 1:\n",
    "            image = cv2.imread(self.images_fps[i//4])[max(0,xs):512+max(0,xs), 512+min(0,ys):1024+min(0,ys)]\n",
    "            mask = cv2.imread(self.masks_fps[i//4], 0)[max(0,xs):512+max(0,xs), 512+min(0,ys):1024+min(0,ys)]\n",
    "        elif i%4 == 2:\n",
    "            image = cv2.imread(self.images_fps[i//4])[512+min(0,xs):1024+min(0,xs):, max(0,ys):512+max(0,ys)]\n",
    "            mask = cv2.imread(self.masks_fps[i//4], 0)[512+min(0,xs):1024+min(0,xs), max(0,ys):512+max(0,ys)]\n",
    "        elif i%4 ==3:\n",
    "            image = cv2.imread(self.images_fps[i//4])[512+min(0,xs):1024+min(0,xs), 512+min(0,ys):1024+min(0,ys)]\n",
    "            mask = cv2.imread(self.masks_fps[i//4], 0)[512+min(0,xs):1024+min(0,xs), 512+min(0,ys):1024+min(0,ys)]\n",
    "            \n",
    "            \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # extract certain classes from mask (e.g. precipitates)\n",
    "        masks = [(np.isin(mask, v)) for v in self.class_values.values()]\n",
    "        mask = np.stack(masks, axis=-1).astype('float')\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)*4\n",
    "\n",
    "#THIS IS THE QUARTERED DATASET WITH 512X512 INSTEAD OF 1024X1024\n",
    "class Dataset_1_4(BaseDataset):\n",
    "    \"\"\"Read images, apply augmentation and preprocessing transformations.\n",
    "    Modified from https://github.com/qubvel/segmentation_models.pytorch\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_values (dict): values of classes to extract from segmentation mask. \n",
    "            Each dictionary value can be an integer or list that specifies the mask\n",
    "            values that belong to the class specified by the corresponding dictionary key.\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir, \n",
    "            class_values,\n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.ids = os.listdir(images_dir)\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
    "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
    "        \n",
    "        self.class_values = class_values\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read data\n",
    "        if i%4 == 0:\n",
    "            image = cv2.imread(self.images_fps[i//4])[:512, :512]\n",
    "            mask = cv2.imread(self.masks_fps[i//4], 0)[:512, :512]\n",
    "        elif i%4 == 1:\n",
    "            image = cv2.imread(self.images_fps[i//4])[:512, 512:]\n",
    "            mask = cv2.imread(self.masks_fps[i//4], 0)[:512, 512:]\n",
    "        elif i%4 == 2:\n",
    "            image = cv2.imread(self.images_fps[i//4])[512:, :512]\n",
    "            mask = cv2.imread(self.masks_fps[i//4], 0)[512:, :512]\n",
    "        elif i%4 ==3:\n",
    "            image = cv2.imread(self.images_fps[i//4])[512:, 512:]\n",
    "            mask = cv2.imread(self.masks_fps[i//4], 0)[512:, 512:]\n",
    "            \n",
    "            \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "\n",
    "        # extract certain classes from mask (e.g. cars)\n",
    "        masks = [(np.isin(mask, v)) for v in self.class_values.values()]\n",
    "        mask = np.stack(masks, axis=-1).astype('float')\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "        #albu.Resize(height,width),\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "\n",
    "        #albu.ShiftScaleRotate(scale_limit=0.3, rotate_limit=25, shift_limit=0.1, p=1, border_mode=0),\n",
    "\n",
    "        \n",
    "        #albu.RandomCrop(height=512, width=512, p=0.25),\n",
    "        #albu.PadIfNeeded(min_height=height, min_width=320, always_apply=True, border_mode=0),\n",
    "        #albu.IAAPerspective(p=0.5),\n",
    "        \n",
    "        \n",
    "        albu.IAAAdditiveGaussianNoise(p=0.2),\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                #albu.CLAHE(p=1),\n",
    "                albu.RandomBrightness(p=1),\n",
    "                #albu.RandomGamma(p=1),\n",
    "            ],\n",
    "            p=0.25,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.IAASharpen(p=1),\n",
    "                albu.Blur(blur_limit=2, p=1),\n",
    "                #albu.MotionBlur(blur_limit=3, p=1),\n",
    "            ],\n",
    "            p=0.25,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.RandomContrast(p=1)#,\n",
    "                #albu.HueSaturationValue(p=1),\n",
    "            ],\n",
    "            p=0.25,\n",
    "        ),\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        #albu.Resize(height,width)\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.__name__ = 'DiceBCELoss'\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        Dice_BCE = self.weight * BCE + (1-self.weight) * dice_loss\n",
    "        \n",
    "        return Dice_BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_module_from_state_dict(state_dict):\n",
    "    \"\"\"Removes 'module.' from nn.Parallel models.  \n",
    "    If module does not exist it just returns the state dict\"\"\"\n",
    "    if list(state_dict.keys())[0].startswith('module'):\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in state_dict.items():\n",
    "            name = k[7:] # remove `module.`\n",
    "            new_state_dict[name] = v           \n",
    "        return new_state_dict\n",
    "    elif list(state_dict.keys())[0].startswith('features.module'):\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in state_dict.items():\n",
    "            name = k[:9] + k[9+7:] # remove `module.`\n",
    "            if name.startswith('features.'):\n",
    "                new_state_dict[name] = v\n",
    "        return new_state_dict\n",
    "    else:\n",
    "        return state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(decoder, encoder, encoder_weights, class_values, device='cuda', lr=2e-4, lr_decay=0.01,\n",
    "               batch_size=20, val_batch_size=12, num_workers=0, patience=30, save_folder='./', multi_gpu=False, step_lr=None):\n",
    "    print(decoder, encoder, encoder_weights)\n",
    "    # setup and check parameters\n",
    "    assert len(class_values) != 2, \"Two classes is binary classification.  Just specify the posative class value\"\n",
    "    activation = 'softmax2d' if len(class_values) > 1 else 'sigmoid' #'softmax2d' for multicalss segmentation\n",
    "    initial_weights = None if encoder_weights is None else 'imagenet'\n",
    "    # load model\n",
    "    \n",
    "    if initial_weights == 'imagenet' and ('dpn68b' in encoder or 'dpn92' in encoder or 'dpn137' in encoder\n",
    "                                         or 'dpn107' in encoder):\n",
    "        initial_weights = 'imagenet+5k'\n",
    "    \n",
    "    try:\n",
    "        model = getattr(smp, decoder)(encoder_name=encoder, \n",
    "                                      encoder_weights=initial_weights,\n",
    "                                      classes=len(class_values),\n",
    "                                      activation=activation)\n",
    "    except ValueError: #certain encoders do not support encoder dilation\n",
    "        if decoder == 'DeepLabV3Plus':\n",
    "            print('\\n\\n%s does not support dilated mode needed for %s. Skipping.\\n\\n' %(encoder, decoder))\n",
    "            return\n",
    "        else:\n",
    "            model = getattr(smp, decoder)(encoder_name=encoder, \n",
    "                                          encoder_weights=initial_weights,\n",
    "                                          classes=len(class_values),\n",
    "                                          activation=activation,\n",
    "                                          encoder_dilation=False)\n",
    "        \n",
    "        \n",
    "    # load pretrained weights \n",
    "    if encoder_weights in ['microscopynet', 'microscopynet_fromscratch']:\n",
    "        # load the saved state dict\n",
    "        try:\n",
    "            if encoder_weights == 'microscopynet':\n",
    "                path = os.path.join(MICRO_MODELS_DIR, microscopynet_weights[encoder])\n",
    "            else:\n",
    "                path = os.path.join(MICRO_MODELS_DIR, microscopynet_fromscratch_weights[encoder])\n",
    "        except KeyError:\n",
    "            print('\\n\\nNo pretrained %s weights for %s encoder!!\\n\\n' %(encoder, encoder_weights))\n",
    "            return\n",
    "        state_dict = torch.load(path)['state_dict']    \n",
    "        \n",
    "        # remove module. from keys if trained with DataParallel\n",
    "        state_dict = remove_module_from_state_dict(state_dict)\n",
    "        \n",
    "        # fix last_linear.bias and last_linear weigths for xception pretrained\n",
    "        if encoder == 'xception':\n",
    "            new_state_dict = OrderedDict()\n",
    "            for k, v in state_dict.items():\n",
    "                name = k.replace('last_linear', 'fc')\n",
    "                new_state_dict[name] = v    \n",
    "            state_dict = new_state_dict\n",
    "        \n",
    "        model.encoder.load_state_dict(state_dict)\n",
    "        \n",
    "    if multi_gpu:\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    # create dataloaders\n",
    "    try:\n",
    "        preprocessing_fn = smp.encoders.get_preprocessing_fn(encoder, 'imagenet')\n",
    "    except ValueError:\n",
    "        preprocessing_fn = smp.encoders.get_preprocessing_fn(encoder, 'imagenet+5k')\n",
    "        \n",
    "    train_dataset = Dataset_1_4_shift(\n",
    "        x_train_dir, \n",
    "        y_train_dir, \n",
    "        augmentation=get_training_augmentation(), \n",
    "        preprocessing=get_preprocessing(preprocessing_fn),\n",
    "        class_values=class_values,\n",
    "    )\n",
    "\n",
    "    valid_dataset = Dataset_1_4(\n",
    "        x_valid_dir, \n",
    "        y_valid_dir, \n",
    "        augmentation=get_validation_augmentation(), \n",
    "        preprocessing=get_preprocessing(preprocessing_fn),\n",
    "        class_values=class_values,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                              shuffle=True, num_workers=num_workers, pin_memory=True)  \n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=val_batch_size, \n",
    "                              shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    \n",
    "    loss = DiceBCELoss(weight=0.7)\n",
    "\n",
    "    metrics = [smp.utils.metrics.IoU(threshold=0.5),]\n",
    "    \n",
    "    optimizer = torch.optim.Adam([ \n",
    "        dict(params=model.parameters(), lr=lr),])\n",
    "    \n",
    "    # create epoch runners \n",
    "    # it is a simple loop of iterating over dataloader`s samples\n",
    "    train_epoch = smp.utils.train.TrainEpoch(\n",
    "        model, \n",
    "        loss=loss, \n",
    "        metrics=metrics, \n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    valid_epoch = smp.utils.train.ValidEpoch(\n",
    "        model, \n",
    "        loss=loss, \n",
    "        metrics=metrics, \n",
    "        device=device,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    patience_step = 0\n",
    "    max_score = 0\n",
    "    best_epoch = 0\n",
    "    epoch = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    state = {'encoder': encoder,\n",
    "             'decoder': decoder,\n",
    "             'train_loss': [],\n",
    "             'valid_loss': [],\n",
    "             'train_iou': [],\n",
    "             'valid_iou': [],\n",
    "             'max_score': 0,\n",
    "             'class_values': class_values\n",
    "            }\n",
    "    \n",
    "    while True:\n",
    "        t = time.time() - t0\n",
    "        print('\\nEpoch: {}, lr: {:0.8f}, time: {:0.2f} seconds, patience step: {}, best iou: {:0.4f}'.format(\n",
    "            epoch, lr, t, patience_step, max_score))\n",
    "        t0 = time.time()\n",
    "        train_logs = train_epoch.run(train_loader)\n",
    "        valid_logs = valid_epoch.run(valid_loader)\n",
    "\n",
    "        # update the state\n",
    "        state['epoch'] = epoch + 1\n",
    "        state['state_dict'] = model.state_dict()\n",
    "        state['optimizer'] = optimizer.state_dict()\n",
    "        state['train_loss'].append(train_logs['DiceBCELoss'])\n",
    "        state['valid_loss'].append(valid_logs['DiceBCELoss'])\n",
    "        state['train_iou'].append(train_logs['iou_score'])\n",
    "        state['valid_iou'].append(valid_logs['iou_score'])\n",
    "        \n",
    "        # save the model\n",
    "        #torch.save(state, os.path.join(save_folder, 'checkpoint.pth.tar'))\n",
    "        \n",
    "        # do something (save model, change lr, etc.)\n",
    "        if max_score < valid_logs['iou_score']:\n",
    "            patience_step = 0\n",
    "            max_score = valid_logs['iou_score']\n",
    "            best_epoch = epoch + 1\n",
    "#             shutil.copyfile(os.path.join(save_folder, 'checkpoint.pth.tar'), \n",
    "#                             os.path.join(save_folder, 'model_best.pth.tar'))\n",
    "            torch.save(state, os.path.join(save_folder, 'model_best.pth.tar'))\n",
    "            print('Best model saved!')\n",
    "        \n",
    "        else:\n",
    "            patience_step += 1\n",
    "\n",
    "        \n",
    "        # Increment the epoch and decay the learning rate\n",
    "        epoch += 1\n",
    "        lr = optimizer.param_groups[0]['lr'] * (1-lr_decay)\n",
    "        optimizer.param_groups[0]['lr'] = lr\n",
    "        \n",
    "        if step_lr is not None and epoch == 25:\n",
    "            lr = step_lr\n",
    "            optimizer.param_groups[0]['lr'] = lr\n",
    "            \n",
    "        # Use early stopping if there has not been improvment in a while\n",
    "        if patience_step > patience:\n",
    "            print('\\n\\nTraining done!  No improvement in {} epochs. Saving final model'.format(patience))\n",
    "            shutil.copyfile(os.path.join(save_folder, 'model_best.pth.tar'), \n",
    "                            os.path.join(save_folder, '{}__{}__{}__{}__{:.3f}.pth.tar'.format(\n",
    "                                decoder, encoder, encoder_weights, best_epoch, max_score)))\n",
    "            break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 1st set with all decoders, all pretraining weights, and limited encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoders = ['Unet', 'FPN', 'Linknet', 'PSPNet', 'PAN', 'DeepLabV3Plus']\n",
    "# encoders_no_run = ['resnext101_32x32d', 'resnext101_32x48d', 'efficientnet-b6', \n",
    "#              'efficientnet-b7', 'resnext101_32x16d', 'resnext101_32x32d',\n",
    "#             'resnext101_32x48d', 'vgg16', 'vgg19', 'vgg19_bn', 'vgg13', 'vgg11'] \n",
    "# encoders = [e for e in smp.encoders.get_encoder_names() if e not in encoders_no_run]\n",
    "encoders = [key for key in microscopynet_fromscratch_weights]\n",
    "encoder_weights = [None, 'imagenet', 'microscopynet', 'microscopynet_fromscratch']\n",
    "class_values = {'background': 0,\n",
    "                'oxide': 1,\n",
    "                'crack': 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e, d, ew in product(encoders, decoders, encoder_weights):\n",
    "    train_model(d, e, ew, class_values, \n",
    "                save_folder='./segmentation_models/ebc_exp1', \n",
    "                multi_gpu=True,\n",
    "                num_workers=2,\n",
    "                patience=30,\n",
    "               lr_decay=0.00,\n",
    "               step_lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 2nd set with all decoders, just imagenet and microscopynet weights, and all encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepLabV3Plus xception imagenet\n",
      "\n",
      "\n",
      "xception does not support dilated mode needed for DeepLabV3Plus. Skipping.\n",
      "\n",
      "\n",
      "DeepLabV3Plus xception microscopynet\n",
      "\n",
      "\n",
      "xception does not support dilated mode needed for DeepLabV3Plus. Skipping.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoders = ['Unet', 'FPN', 'Linknet', 'PSPNet', 'PAN', 'DeepLabV3Plus']\n",
    "\n",
    "old_encoders = [key for key in microscopynet_fromscratch_weights]\n",
    "encoders_no_run = ['resnext101_32x32d', 'resnext101_32x48d', 'efficientnet-b6', \n",
    "             'efficientnet-b7', 'resnext101_32x16d', 'resnext101_32x32d',\n",
    "            'resnext101_32x48d', 'vgg16', 'vgg19', 'vgg19_bn', 'vgg13', 'vgg11',\n",
    "                  'timm-efficientnet-b0', 'timm-efficientnet-b1', 'timm-efficientnet-b2', \n",
    "                   'timm-efficientnet-b3', 'timm-efficientnet-b4', 'timm-efficientnet-b5', \n",
    "                   'timm-efficientnet-b6', 'timm-efficientnet-b7', 'timm-efficientnet-b8', 'timm-efficientnet-l2'] \n",
    "encoders_no_run = encoders_no_run + old_encoders\n",
    "encoders = [e for e in smp.encoders.get_encoder_names() if e not in encoders_no_run]\n",
    "\n",
    "encoder_weights = ['imagenet', 'microscopynet']\n",
    "class_values = {'background': 0,\n",
    "                'oxide': 1,\n",
    "                'crack': 2}\n",
    "\n",
    "i = 0\n",
    "for i, (e, d, ew) in enumerate(product(encoders, decoders, encoder_weights)):\n",
    "    if i >= 370:\n",
    "        train_model(d, e, ew, class_values, \n",
    "                    save_folder='./segmentation_models/ebc_exp1', \n",
    "                    multi_gpu=True,\n",
    "                    num_workers=2,\n",
    "                    patience=30,\n",
    "                   lr_decay=0.00,\n",
    "                   step_lr=1e-5)\n",
    "        #print(i, e, d, ew)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model = torch.load('./segmentation_models/ebc_exp1/model_best.pth.tar')\n",
    "best_model = torch.load('./segmentation_models/ebc_exp1/Unet__inceptionresnetv2__microscopynet__200__0.981.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for key in best_model:\n",
    "#     print(key)\n",
    "x = range(len(best_model['valid_loss']))    \n",
    "plt.plot(x, best_model['valid_loss'], label='val_loss')\n",
    "plt.plot(x, best_model['train_loss'], label='train_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#print(best_model['train_iou'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inceptionresnetv2 Unet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = 'cuda'\n",
    "decoder = best_model['decoder']\n",
    "encoder = best_model['encoder']\n",
    "#class_values = best_model['class_values']\n",
    "class_values = {'background': 0,\n",
    "                'oxide': 1,\n",
    "                'crack': 2}\n",
    "activation = 'softmax2d' if len(class_values) > 1 else 'sigmoid' #'softmax2d' for multicalss segmentation\n",
    "print(encoder, decoder)\n",
    "try:\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(encoder, 'imagenet')\n",
    "except ValueError:\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(encoder, 'imagenet+5k')\n",
    "model = getattr(smp, decoder)(encoder_name=encoder, \n",
    "                                      encoder_weights=None,\n",
    "                                      classes=len(class_values),\n",
    "                                      activation=activation)\n",
    "\n",
    "model.load_state_dict(remove_module_from_state_dict(best_model['state_dict']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test dataset\n",
    "test_dataset = Dataset_1_4(\n",
    "    x_test_dir, \n",
    "    y_test_dir, \n",
    "    augmentation=get_validation_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    class_values=class_values,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset)\n",
    "\n",
    "# test dataset without transformations for image visualization\n",
    "test_dataset_vis = Dataset_1_4(\n",
    "    x_test_dir, y_test_dir, \n",
    "    class_values=class_values,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  33%|      | 4/12 [00:00<00:00, 14.05it/s, DiceBCELoss - 0.5461, iou_score - 0.982] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jstuckne/.conda/envs/fastai/lib/python3.7/site-packages/torch/nn/functional.py:1350: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid: 100%|| 12/12 [00:00<00:00, 15.19it/s, DiceBCELoss - 0.5462, iou_score - 0.9814]\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on test set\n",
    "loss = DiceBCELoss(weight=0.7)\n",
    "metrics = [smp.utils.metrics.IoU(threshold=0.5),]\n",
    "test_epoch = smp.utils.train.ValidEpoch(\n",
    "    model=model,\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    "    device='cuda',\n",
    ")\n",
    "\n",
    "logs = test_epoch.run(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    n = np.random.choice(len(test_dataset))\n",
    "    \n",
    "    image_vis = test_dataset_vis[n][0].astype('uint8')\n",
    "    image, gt_mask = test_dataset[n]\n",
    "    \n",
    "    gt_mask_background = gt_mask[0].squeeze()\n",
    "    gt_mask_oxide = gt_mask[1].squeeze()\n",
    "    gt_mask_crack = gt_mask[2].squeeze()\n",
    "\n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "    pr_mask = model.predict(x_tensor)\n",
    "\n",
    "\n",
    "    pr_mask = (pr_mask.squeeze().cpu().numpy().round())\n",
    "    pr_mask_oxide = pr_mask[1].squeeze()\n",
    "    pr_mask_crack = pr_mask[2].squeeze()\n",
    "    \n",
    "    \n",
    "    \n",
    "    pr_mask_background = pr_mask[0].squeeze()\n",
    "        \n",
    "    visualize(\n",
    "        image=image_vis, \n",
    "        ground_truth_oxide=gt_mask_oxide,\n",
    "        predicted_oxide=pr_mask_oxide,\n",
    "        ground_truth_crack=gt_mask_crack,\n",
    "        predicted_crack=pr_mask_crack\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
